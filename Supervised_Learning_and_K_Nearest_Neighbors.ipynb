{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado Supervisionado e K Vizinhos mais Próximos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Nós utilizaremos a base de dados de taxa de rejeição da indústria de telecomunicações para essa lista de exercícios. A base de dados tem o nome `Orange_Telecom_Churn_Data.csv`. Nesse notebook carregaremos a base de dados, faremos algum pré-processamento e usaremos a técnica k-NN para predizer a taxa de rejeição baseado nas características mensuradas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "\n",
    "* Comece importando os dados através do pandas. Examine as colunas e os dados\n",
    "* Note que os dados contêm estado, código de área e telefone. Você acha que esses atributos são interessantes para construir nosso modelo de classificação? Por que?\n",
    "\n",
    "Não utilizaremos esses atributos para essa base, então eles podem ser removidos do dataframe (método `drop`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "arquivo = 'data/Orange_Telecom_Churn_Data.csv'\n",
    "\n",
    "data = pd.read_csv(arquivo)\n",
    "#print (data)\n",
    "df1 = data.drop(columns=['state', 'area_code', 'phone_number'])\n",
    "print (df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "\n",
    "* Note que algumas das colunas são categóricas e algumas são *float*. Esses atributos precisam ser numéricos para usar os algoritmos que aprenderemos no curso.\n",
    "* Finalmente, o algoritmo K-Vizinhos mais próximos necessita de dados escalonados. Escalone os dados utilizando um dos métodos aprendidos em aula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "\n",
    "bin_cols = ['intl_plan', 'voice_mail_plan', 'churned']\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "#stdSc = StandardScaler()StratifiedShuffleSplit\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for column in bin_cols:\n",
    "    df1[column] = lb.fit_transform(df1[column])\n",
    "\n",
    "#df1[df1.columns] = stdSc.fit_transform(df1)\n",
    "df1[df1.columns] = scaler.fit_transform(df1)\n",
    "\n",
    "print (df1)\n",
    "#df[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3\n",
    "\n",
    "* Separe as colunas de atributos (todas menos `churned`) da  coluna de rótulo (`churned`). Isso criará duas tabelas.\n",
    "* Aplique o método `.fit()` do K-nearest neighbors com um valor de `n_neighbors=3` para essa base de dados e verifique o resultado com o método `.predict()` na mesma base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_cols = [x for x in data.columns if x != 'churned']\n",
    "\n",
    "X_data = df1[x_cols]\n",
    "y_data = df1['churned']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kNN_classifier = KNeighborsClassifier(n_neighbors = 3)\n",
    "kNN_classifier.fit(X_data, y_data)\n",
    "predicted_dataset = kNN_classifier.predict(X_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4\n",
    "\n",
    "Ainda não ensinamos formas de medir o erro, mas a acurácia é um conceito simples de entender--é a porcentagem de rótulos que foram corretamente classificados:\n",
    "\n",
    "$acc = \\frac{corretos}{total}$\n",
    "\n",
    "* Escreva uma função para calcular a acurácia usando os rótulos verdadeiros e os preditos.\n",
    "* Usando a função, calcule a acurácia do K-nn nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def accuracy(real, predict):\n",
    "    return \"datasets do not have the same length.\" if len(real) != len(predict) else reduce(lambda x, y: x + y, map(lambda tup: 1 if tup[0] == tup[1] else 0, zip(y_data, predict))) / len(real)\n",
    "\n",
    "accuracy(y_data, predicted_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5\n",
    "\n",
    "* Repita a aplicação do modelo K-nearest neighbors mas com o parâmetro `weights=distance`. Calcule a acurácia da função criada por você.\n",
    "* Repita a aplicação do modelo com `n_neighbors=3`, `weights=uniform` e `p=1` para utilizar a distância de Manhattan. Verifique os resultados.\n",
    "\n",
    "Quando as distâncias ponderadas são utilizadas para a parte 1 dessa questão, vocês obterão acurácia de 1.0. Por que isso ocorre? *Dica:* o KNN usa diretamente os dados para fazer a predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kNN_inverse_distance_classifier = KNeighborsClassifier(n_neighbors = 3, weights = 'distance')\n",
    "kNN_inverse_distance_classifier.fit(X_data, y_data)\n",
    "predicted_dataset_by_inverse_distance = kNN_inverse_distance_classifier.predict(X_data)\n",
    "\n",
    "print(f\"Acurácia da predição pelo inverso da distância: {accuracy(y_data, predicted_dataset_by_inverse_distance)}\")\n",
    "\n",
    "kNN_manhattan_classifier = KNeighborsClassifier(n_neighbors = 3, weights = 'uniform', p = 1)\n",
    "kNN_manhattan_classifier.fit(X_data, y_data)\n",
    "predicted_dataset_by_manhattan_distance = kNN_manhattan_classifier.predict(X_data)\n",
    "\n",
    "print(f\"Acurácia da predição pela distância de Manhattan: {accuracy(y_data, predicted_dataset_by_manhattan_distance)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 6\n",
    "\n",
    "* Teste o modelo KNN utilizando valores de `n_neighbors` na faixa de 1 a 20. Deixe o restante dos parâmetros como o padrão. Armazene os valores de vizinhos e acurácia em uma lista no formato [(k, acuracia)].\n",
    "* Plote um gráfico do tipo *scatter* da acurácia vs vizinhos. O que acontece com `n_neighbors=1`? Por que isso ocorre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_list = list()\n",
    "\n",
    "'''escreva no bloco abaixo'''\n",
    "for k in range (1, 21):\n",
    "    kNN_classifier = KNeighborsClassifier(n_neighbors = k)\n",
    "    kNN_classifier.fit(X_data, y_data)\n",
    "    predicted_dataset = kNN_classifier.predict(X_data)\n",
    "    score_list.append((k, accuracy(y_data, predicted_dataset)))\n",
    "'''fim do código'''\n",
    "\n",
    "score_df = pd.DataFrame(score_list, columns=['k', 'accuracy'])\n",
    "score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries to make the plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.set_style('ticks')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "ax = score_df.plot(x = 'k', y = 'accuracy', kind = 'scatter', grid = True)\n",
    "\n",
    "ax.set(xlabel='k', ylabel='accuracy')\n",
    "ax.set_xticks(range(1, 21));\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "Linear_Regression_and_K_Nearest_Neighbors_Exercises-ANSWERS",
  "notebookId": 2125319687183902
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
